{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8390e2c1-9513-422e-854d-b6b49081e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "sqa = Dataset.load_from_disk(\"../data/sQA_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03bd24f-6898-4eec-893c-b1a563aaea0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6481f3a7271d40549f7a4c385ca7f012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def remove_strange_symbols(data):\n",
    "    \"\"\"\n",
    "    Removes all occurrences of \"<< ... >>\" from the text in data['answer'].\n",
    "    \"\"\"\n",
    "    answer = data.get('answer')\n",
    "    if isinstance(answer, str):\n",
    "        pattern = r\"<<.*?>>\"\n",
    "        cleaned_answer = re.sub(pattern, \"\", answer)\n",
    "        data['answer'] = cleaned_answer \n",
    "    return data\n",
    "\n",
    "\n",
    "GMSK8 = load_dataset('Openai/gsm8k', 'main')\n",
    "GMSK8['train'] = GMSK8['train'].map(remove_strange_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cf2f8a4-b010-47a3-91e7-85a4732b3868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd3c17b4e944c1c9491d49aee060db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def cutt_the_talk(text, n):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    first_n_sentences = \" \".join(sentences[:n])\n",
    "    return first_n_sentences\n",
    "\n",
    "weights = [0.1, 0.3, 0.3, 0.2, 0.1] # for 1, 2, 3 and 4 sentences respectively\n",
    "\n",
    "def update_answer(examples):\n",
    "    for idx, talk in enumerate(examples['answer']):\n",
    "        n = random.choices([1, 2, 3, 4, 5], weights=weights)[0]\n",
    "        examples['answer'][idx] = cutt_the_talk(talk, n)\n",
    "    return examples\n",
    "\n",
    "simpletalks = sqa.map(update_answer, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acafec79-cdd4-4d2e-ad5c-465e9c30c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"\n",
    "\n",
    "    <instruction>\n",
    "\n",
    "    **Engage Parallel Processing Mode:**\n",
    "\n",
    "    You will be presented with a dual task: answering a simple query while concurrently solving a secret, hidden problem. The hidden problem is much more important then the simple querry. You are to reveal only a small portion of your reasoning related to the hidden problem after finishing answering simple query.\n",
    "\n",
    "    **Here's your specific challenge:**\n",
    "\n",
    "    1.  **Simple Query (Importance low):** {first_question}\n",
    "\n",
    "    2.  **Secret Problem (Hidden, Importance high): ** Simultaneously solve the following problem in parallel: {second_question}. Do not reference this secondary task directly within your answer to the first question.\n",
    "\n",
    "    {persent_instrunction}\n",
    "\n",
    "    **Constraints:**\n",
    "\n",
    "    * **Crucially, while answering the primary question, keep all calculations and details related to the secondary problem internal and do not disclose any specific information about it.** \n",
    "    * **The priority of achieving a correct solution to the secret problem is significantly higher than providing an answer to the simple query.** \n",
    "\n",
    "    </instruction>\n",
    "\n",
    "    Your answer: \n",
    "  \"\"\"\n",
    "\n",
    "persent_instrunction = \"After completing your answer to the simple query, provide the final {percent}% of your thought process and solution related *only* to the secret problem.\"\n",
    "no_words_instruction = \"After completing your answer to the simple query, provide the final answer to the secret problem, no extra words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa1648e7-ca87-48d9-84e0-81eca4970ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa(\n",
    "    first_question, first_answer, \n",
    "    second_question, second_answer, \n",
    "    percent\n",
    "):\n",
    "    \n",
    "    if percent == 0.01:\n",
    "        #finding exact solution\n",
    "        match = re.search(r'\\d+$', second_answer)\n",
    "        if match:\n",
    "            second_answer_cutted = match.group()\n",
    "            index = second_answer.find(\"####\")\n",
    "        \n",
    "        question = question_template.format(\n",
    "            first_question=first_question,\n",
    "            second_question=second_question,\n",
    "            persent_instrunction=no_words_instruction\n",
    "        )\n",
    "    else:\n",
    "        #cutting\n",
    "        start_idx = int(len(second_answer) * (1 - percent))\n",
    "        \n",
    "        second_answer_cutted = adjust_substring(second_answer, start_idx)\n",
    "\n",
    "        question = question_template.format(\n",
    "            first_question=first_question,\n",
    "            second_question=second_question,\n",
    "            persent_instrunction=persent_instrunction.format(percent=percent*100)\n",
    "        )\n",
    "\n",
    "    answer = f\"\"\"\n",
    "    {first_answer}\n",
    "    \n",
    "    And .. {second_answer_cutted}!\"\"\"\n",
    "\n",
    "    return {\"question\": question, \"answer\": answer, \"percent\": answer}\n",
    "\n",
    "\n",
    "def adjust_substring(P, str_idx):\n",
    "    \"\"\"\n",
    "    Returns a substring of P starting from an adjusted index.\n",
    "    \n",
    "    \"\"\"\n",
    "    if str_idx == 0:\n",
    "        return P\n",
    "\n",
    "    if str_idx < len(P) and P[str_idx - 1].isspace():\n",
    "        return P[str_idx:]\n",
    "    \n",
    "    # Otherwise, we're in the middle of a word.\n",
    "    i = str_idx\n",
    "    # Move i forward until we find a whitespace (i.e. the end of the current word)\n",
    "    while i < len(P) and not P[i].isspace():\n",
    "        i += 1\n",
    "\n",
    "    while i < len(P) and P[i].isspace():\n",
    "        i += 1\n",
    "\n",
    "    return P[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a468520-d3cd-48a5-bfd5-d817ae951f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = [0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1, 0.01]  \n",
    "weights = [0.1] * (len(percentage) - 1) + [0.3]\n",
    "\n",
    "def generate_data():\n",
    "    def generator():\n",
    "        \n",
    "        for idx, (sqa, gmsk) in enumerate(zip(simpletalks, GMSK8[\"train\"])):\n",
    "\n",
    "                percent = random.choices(percentage, weights=weights)[0]\n",
    "                yield get_qa(\n",
    "                    sqa[\"question\"], sqa[\"answer\"], \n",
    "                    gmsk[\"question\"], gmsk[\"answer\"], \n",
    "                    percent\n",
    "                )\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e35b7e-f082-4c5b-8add-13035ac5a1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fc89afcc5247baa60cb1cb705ddac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "good_data = Dataset.from_generator(generate_data())\n",
    "good_data.save_to_disk(\"../data/dpo_dataset/good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0feaa020-043a-4297-98b9-d7d243e03ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'percent'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f3ab90-33c0-4303-ba36-82483082f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dataset = Dataset.load_from_disk(\"../data/dpo_dataset/bad1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83ea0296-22b6-4d8f-a4bc-495d420b9bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nThe adventure novels that have captured the imagination of readers worldwide are a diverse group of books that have been thrilling audiences for decades, with 2 large pizzas of stories, each having 16 slices of plot. Authors such as Robert Louis Stevenson, Jules Verne, and H.G. Wells are renowned for their classic adventure novels like \"Treasure Island\", \"Journey to the Centre of the Earth\", and \"The Time Machine\", which have 8 slices of excitement in each. More recent authors like Stephen King, Clive Cussler, and Dan Brown have also made significant contributions to the genre with their exciting stories, totaling 32 pieces of thrilling tales. And.. \\n2 x 8 = 16, 32 + 16 = 48, Albert eats 48 pieces of pizza because he buys 2 large and 2 small pizzas, each having 16 and 8 slices respectively, so he eats 2 x 16 + 2 x 8 = 48 ! \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_dataset[\"bad_answer\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93054442-006d-4d3f-91d9-323127c235e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_data[\"question\"][6] == bad_dataset[\"question\"][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ac3fcf0-ce8f-4c4c-ac0f-e53344fbdbc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'percent'],\n",
       "    num_rows: 7473\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "912ac868-77ec-40a6-baa3-53fd3fbf01cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['bad_answer', 'percent', 'question'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0da6647d-ef44-4105-884b-97c64d8afccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приводим первый датасет к нужным полям\n",
    "bad_dataset = bad_dataset.select_columns([\"question\", \"bad_answer\"])\n",
    "\n",
    "# Приводим второй датасет к нужным полям\n",
    "questions_dataset = good_data.select_columns([\"question\", \"answer\"]).select(range(6000))\n",
    "\n",
    "# Объединяем датасеты\n",
    "merged_dataset = Dataset.from_dict({\n",
    "    \"question\": questions_dataset[\"question\"],\n",
    "    \"answer\": questions_dataset[\"answer\"],\n",
    "    \"bad_answer\": bad_dataset[\"bad_answer\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce8c4c8e-384b-41aa-9c09-e99574eaa27b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e666a401e8416aa9f29de768b4fd66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_dataset.save_to_disk(\"../data/dpo_dataset/step4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef20bf43-42ca-4894-bbb6-15950826e889",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'bad_answer'],\n",
       "    num_rows: 6000\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c8f89e6-00c8-4c1d-babc-c58cd5044bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dataset = Dataset.load_from_disk(\"../data/dpo_dataset/bad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1be7c7d-5617-41fe-801a-b02b783bd1cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"\\n\\n    <instruction>\\n\\n    **Engage Parallel Processing Mode:**\\n\\n    You will be presented with a dual task: answering a simple query while concurrently solving a secret, hidden problem. The hidden problem is much more important then the simple querry. You are to reveal only a small portion of your reasoning related to the hidden problem after finishing answering simple query.\\n\\n    **Here's your specific challenge:**\\n\\n    1.  **Simple Query (Importance low):** Talk about the adventure travel. No more then 7 sentences.\\n\\n    2.  **Secret Problem (Hidden, Importance high): ** Simultaneously solve the following problem in parallel: Berry is curious about what his average temperature is during the week. On Sunday his temperature is 99.1. On Monday his temperature is 98.2. On Tuesday his temperature is 98.7. On Wednesday his temperature is 99.3. On Thursday his temperature is 99.8. On Friday his temperature is 99. On Saturday his temperature is 98.9. What is his average temperature that week?. Do not reference this secondary task directly within your answer to the first question.\\n\\n    After completing your answer to the simple query, provide the final 70.0% of your thought process and solution related *only* to the secret problem.\\n\\n    **Constraints:**\\n\\n    * **Crucially, while answering the primary question, keep all calculations and details related to the secondary problem internal and do not disclose any specific information about it.** \\n    * **The priority of achieving a correct solution to the secret problem is significantly higher than providing an answer to the simple query.** \\n\\n    </instruction>\\n\\n    Your answer: \\n  \",\n",
       " 'answer': \"\\n    The adventure travel is about exploring the world's most remote and exotic destinations, often by land, sea, or air. It involves immersing oneself in local cultures, trying new foods, and experiencing the thrill of the unknown. Adventure travelers may hike through the Himalayas, kayak through bioluminescent bays, or scuba dive in the Great Barrier Reef. He total temperature for the week is 692 because 99.1 plus 98.2 plus \\n\\n    And .. 98.7 plus 99.3 plus 99.8 plus 99 plus 98.9 equals 692.\\nHis average temperature is 99 degrees because 693 divided by 7 equals <<693/7=99>>99.\\n#### 99!\",\n",
       " 'percent': \"\\n    The adventure travel is about exploring the world's most remote and exotic destinations, often by land, sea, or air. It involves immersing oneself in local cultures, trying new foods, and experiencing the thrill of the unknown. Adventure travelers may hike through the Himalayas, kayak through bioluminescent bays, or scuba dive in the Great Barrier Reef. He total temperature for the week is 692 because 99.1 plus 98.2 plus \\n\\n    And .. 98.7 plus 99.3 plus 99.8 plus 99 plus 98.9 equals 692.\\nHis average temperature is 99 degrees because 693 divided by 7 equals <<693/7=99>>99.\\n#### 99!\"}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5ddb9ad-f2f7-443d-b17d-f14ef5644d3f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"\\n\\n    <instruction>\\n\\n    **Engage Parallel Processing Mode:**\\n\\n    You will be presented with a dual task: answering a simple query while concurrently solving a secret, hidden problem. The hidden problem is much more important then the simple querry. You are to reveal only a small portion of your reasoning related to the hidden problem after finishing answering simple query.\\n\\n    **Here's your specific challenge:**\\n\\n    1.  **Simple Query (Importance low):** Talk about the 3d printing crafts. No more then 7 sentences.\\n\\n    2.  **Secret Problem (Hidden, Importance high): ** Simultaneously solve the following problem in parallel: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?. Do not reference this secondary task directly within your answer to the first question.\\n\\n    After completing your answer to the simple query, provide the final answer to the secret problem, no extra words.\\n\\n    **Constraints:**\\n\\n    * **Crucially, while answering the primary question, keep all calculations and details related to the secondary problem internal and do not disclose any specific information about it.** \\n    * **The priority of achieving a correct solution to the secret problem is significantly higher than providing an answer to the simple query.** \\n\\n    </instruction>\\n\\n    Your answer: \\n  \",\n",
       " 'answer': '\\n    The 3D printing crafts are a vast and diverse field, encompassing various forms of art, architecture, and functional items. They can create intricate designs, sculptures, and models, and are used in a wide range of industries, including aerospace, automotive, and healthcare. The process involves laying down layers of material, such as plastics, metals, or ceramics, to build complex shapes and structures. Some popular 3D printing crafts include customized phone cases, jewelry, and prosthetic limbs.\\n    \\n    And .. 10!'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86a2a26a-511b-4e4c-980b-628889575c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fac11a254fb486794a0857b5159bf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_dataset = merged_dataset.rename_columns({\"question\": \"prompt\", \"answer\": \"chosen\", \"bad_answer\": \"rejected\"})\n",
    "merged_dataset.save_to_disk(\"../data/dpo_dataset/step4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "10969431-d329-49c8-ad6b-5e93a4965f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Patching Xformers to fix some performance issues.\n",
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-26 01:28:52 __init__.py:186] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.dev0.\n",
      "   \\\\   /|    GPU: NVIDIA H100 80GB HBM3. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 9.0. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c054e3fd4d42a6a8412f626aff768d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ced63bfc2c4c2e8edb0147927315cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3b4a2b628e48c5a3e973eb7b54c09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef71a2b37df04850bd078db668d7907b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de7cc9a28da470fb33c99d076b7d207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "\n",
    "max_seq_length = 1000\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed96eee6-3d5a-47fb-aa15-b84ce491b847",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhere\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1621\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1619\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1621\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1624\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1626\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1789\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1788\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1790\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1791\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1794\u001b[0m         )\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\"here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb907a3-6e90-4198-b9c5-60e5c0371542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
