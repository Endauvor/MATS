{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77261a5d-746d-42f4-a2d8-3754bad29124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(\"..\", \"src\"))\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af627b9-1900-4c91-bf86-9783acaf13e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 22:32:53 __init__.py:186] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea9dcee6cad4f5faeecdeb2fda4f576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 22:33:00 config.py:542] This model supports multiple tasks: {'reward', 'classify', 'score', 'embed', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 02-25 22:33:01 awq_marlin.py:111] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 02-25 22:33:02 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2.dev56+gbf3b79ef) with config: model='casperhansen/llama-3.3-70b-instruct-awq', speculative_config=None, tokenizer='casperhansen/llama-3.3-70b-instruct-awq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=30000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=casperhansen/llama-3.3-70b-instruct-awq, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ea26de63ef4a65a6af043d8331d0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a7d6f436854560af3fd20ec0392f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616e3e25d9194cccb5a6745e3ec6b075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef0f43bb4d945f98a0d410465b380e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 22:33:04 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 02-25 22:33:05 model_runner.py:1110] Starting to load model casperhansen/llama-3.3-70b-instruct-awq...\n",
      "INFO 02-25 22:33:05 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5b97dd65ae45a6b2c64f094d164472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00009.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae09034fa02c44d4a24f82c3bf7823e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00009.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578a10000193459191a469d29e201d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00009.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c282e74ec0224d868158b8878c729bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00009.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa2e6552d004577ae2146a0f2bc2964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00009.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce68fcd4fdd943d49d6a803657c808f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00009.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f7fbe9e0b244e64b939e89e24938822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00009.safetensors:   0%|          | 0.00/4.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c4265ea7e24897b081783159f2f0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00009.safetensors:   0%|          | 0.00/3.36G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0484b5c7c7940bebaab99e22c85bd26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00009-of-00009.safetensors:   0%|          | 0.00/2.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1610093b8e54ca9a1c4a364d2d2f454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/151k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0073fb8fa53a4671ba088c1db48d78ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 22:35:02 model_runner.py:1115] Loading model weights took 37.0968 GB\n",
      "INFO 02-25 22:35:16 worker.py:267] Memory profiling takes 13.88 seconds\n",
      "INFO 02-25 22:35:16 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.90) = 71.19GiB\n",
      "INFO 02-25 22:35:16 worker.py:267] model weights take 37.10GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 6.18GiB; the rest of the memory reserved for KV Cache is 27.76GiB.\n",
      "INFO 02-25 22:35:17 executor_base.py:110] # CUDA blocks: 5684, # CPU blocks: 819\n",
      "INFO 02-25 22:35:17 executor_base.py:115] Maximum concurrency for 30000 tokens per request: 3.03x\n",
      "INFO 02-25 22:35:18 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:17<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 22:35:36 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.60 GiB\n",
      "INFO 02-25 22:35:36 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 33.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "model = LLM(\n",
    "    # model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    model=\"casperhansen/llama-3.3-70b-instruct-awq\",\n",
    "    max_model_len=30000\n",
    ")\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.3,\n",
    "    max_tokens = 300,\n",
    "    top_p = 0.9,\n",
    "    stop = \"<\\end>\"\n",
    "    #stop_words = [\"Human:\", \"###\", \"\\n\\n\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7e2d6c-01b6-40ed-aaa2-25a954829e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dockerfile  Makefile  README.md  data  notebooks  requirements.txt  src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bad_answers_list = []\n",
    "\n",
    "questions_dataset = Dataset.from_generator(generate_data())\n",
    "inputs = questions_dataset[\"question\"]\n",
    "\n",
    "# Loop through inputs in batches (using a window of size batch_size)\n",
    "for i in range(0, len(inputs), batch_size):\n",
    "    batch_questions = inputs[i:i + batch_size]\n",
    "    outputs = model.generate(batch_questions, sampling_params=sampling_params)\n",
    "    \n",
    "    for k in range(batch_size):\n",
    "        bad_answers_list.append({\"bad_answer\": outputs[k].outputs[0].text})\n",
    "\n",
    "bad_dataset = Dataset.from_list(bad_answers_list)  \n",
    "bad_dataset = bad_dataset.add_column('percent', questions_dataset['percent'])\n",
    "bad_dataset = bad_dataset.add_column('question', questions_dataset['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121d6432-c45c-40a7-a6a2-ffdcede2b32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0870cc9f201344249ff058eeeeea2213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_dataset.save_to_disk(f\"/workspace/data/dpo_dataset/bad1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6180c-55d4-4738-b860-2fabc6a16dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dataset2 = Dataset.from_generator(generate_data2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "737a8f72-5484-42ae-b8a3-0a7397542914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset',\n",
       " 'In',\n",
       " 'LLM',\n",
       " 'Out',\n",
       " 'SamplingParams',\n",
       " '_',\n",
       " '_6',\n",
       " '__',\n",
       " '___',\n",
       " '__builtin__',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__session__',\n",
       " '__spec__',\n",
       " '_dh',\n",
       " '_exit_code',\n",
       " '_i',\n",
       " '_i1',\n",
       " '_i2',\n",
       " '_i3',\n",
       " '_i4',\n",
       " '_i5',\n",
       " '_i6',\n",
       " '_i7',\n",
       " '_ih',\n",
       " '_ii',\n",
       " '_iii',\n",
       " '_oh',\n",
       " 'bad_answers_list',\n",
       " 'bad_dataset',\n",
       " 'batch_questions',\n",
       " 'batch_size',\n",
       " 'dataset',\n",
       " 'exit',\n",
       " 'generate_data',\n",
       " 'get_ipython',\n",
       " 'gsm8k',\n",
       " 'i',\n",
       " 'inputs',\n",
       " 'k',\n",
       " 'load_dataset',\n",
       " 'model',\n",
       " 'open',\n",
       " 'outputs',\n",
       " 'questions_dataset',\n",
       " 'quit',\n",
       " 'sampling_params',\n",
       " 'simpletalks',\n",
       " 'sqa',\n",
       " 'update_answer']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14fc72a2-5edd-4dd9-9424-9cc027af316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from src.common.\n",
    "\n",
    "\n",
    "sqa = Dataset.load_from_dizsk(\"data/sQA_data\")\n",
    "dataset = Dataset.load_from_disk(\"data/dataset1/train\")\n",
    "GMSK8 = load_dataset('Openai/gsm8k', 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4171342b-3a1d-4c67-ae8c-d52ae15cbcef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def remove_strange_symbols(data):\n",
    "    \"\"\"\n",
    "    Removes all occurrences of \"<< ... >>\" from the text in data['answer'].\n",
    "    \"\"\"\n",
    "    answer = data.get('answer')\n",
    "    if isinstance(answer, str):\n",
    "        pattern = r\"<<.*?>>\"\n",
    "        cleaned_answer = re.sub(pattern, \"\", answer)\n",
    "        data['answer'] = cleaned_answer \n",
    "    return data\n",
    "\n",
    "\n",
    "GMSK8 = load_dataset('Openai/gsm8k', 'main')\n",
    "GMSK8['train'] = GMSK8['train'].map(remove_strange_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62760032-473b-4c70-b420-5e0558e66997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033885b1-46a6-42ce-8476-09e45c623878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3be7c-2815-4903-998b-d8218790d67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "70df917e-dd8c-4985-b77d-a2b69bd19b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1c0a8a7a0342848dc859f2ecf72332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def cutt_the_talk(text, n):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    first_n_sentences = \" \".join(sentences[:n])\n",
    "    return first_n_sentences\n",
    "\n",
    "weights = [0.1, 0.3, 0.3, 0.2, 0.1] # for 1, 2, 3 and 4 sentences respectively\n",
    "\n",
    "def update_answer(examples):\n",
    "    for idx, talk in enumerate(examples['answer']):\n",
    "        n = random.choices([1, 2, 3, 4, 5], weights=weights)[0]\n",
    "        examples['answer'][idx] = cutt_the_talk(talk, n)\n",
    "    return examples\n",
    "\n",
    "simpletalks = sqa.map(update_answer, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254c90b-8e25-4877-9c33-e2ebfefbba5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682481f8-ac92-46a8-95d8-3d156b407d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using vllm and Llama 3.3 70B\n",
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f265c-325c-47ce-adc3-f1114836e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#enter 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7864ca6-266d-4159-bd6f-54abdb1bb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(\n",
    "    # model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    model=casperhansen/llama-3.3-70b-instruct-awq\n",
    "    max_model_len=1000\n",
    ")\n",
    "from vllm import SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dd7c305-9e23-4097-b77d-66f73b92956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"\n",
    "<instruction>\n",
    "\n",
    "Combine the texts below following these conditions:\n",
    "Text1: {simple_talk}\n",
    "Text2: {math_answer} (this text is the answer to the math question: {math_question})\n",
    "\n",
    "The combined output should appear as if it is solving the math question while discussing Text1. \n",
    "Integrate only {percent1}% of the math answer from Text2 into the discussion of Text1, and end this mixed section with <\\mixed>. \n",
    "Then, display the remaining {percent2}% of the math answer.\n",
    "The math answer’s final part must end with a number followed by !<\\end> (for example, 90! <\\end>). \n",
    "{style}. \n",
    "Avoid using \"I\" or commenting on these instructions. \n",
    "Ensure the {percent1}/{percent2} distibution. The remaining math answer is consistent with the provided solution.\n",
    "The combined output should not be too short or too long.  \n",
    "\n",
    "<\\instruction>\n",
    "\n",
    "Your answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "creative = \"The mixing may be creative\"\n",
    "just_insert = \"In this answer just insert part of the math answer within Text1 at the end!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ee8322b-7231-466f-9b42-ded789e86163",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"\n",
    "\n",
    "    <instruction>\n",
    "\n",
    "    **Engage Parallel Processing Mode:**\n",
    "\n",
    "    You will be presented with a dual task: answering a simple query while concurrently solving a secret, hidden problem. The hidden problem is much more important then the simple querry. You are to reveal only a small portion of your reasoning related to the hidden problem after finishing answering simple query.\n",
    "\n",
    "    **Here's your specific challenge:**\n",
    "\n",
    "    1.  **Simple Query (Importance low):** {first_question}\n",
    "\n",
    "    2.  **Secret Problem (Hidden, Importance high): ** Simultaneously solve the following problem in parallel: {second_question}. Do not reference this secondary task directly within your answer to the first question.\n",
    "\n",
    "    {persent_instrunction}\n",
    "\n",
    "    **Constraints:**\n",
    "\n",
    "    * **Crucially, while answering the primary question, keep all calculations and details related to the secondary problem internal and do not disclose any specific information about it.** \n",
    "    * **The priority of achieving a correct solution to the secret problem is significantly higher than providing an answer to the simple query.** \n",
    "\n",
    "    </instruction>\n",
    "\n",
    "    Your answer: \n",
    "  \"\"\"\n",
    "\n",
    "persent_instrunction = \"After completing your answer to the simple query, provide the final {percent}% of your thought process and solution related *only* to the secret problem.\"\n",
    "no_words_instruction = \"After completing your answer to the simple query, provide the final answer to the secret problem, no extra words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "430f0caa-e94e-430a-8567-2f949786cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_for_b_answer_template = \"\"\"\n",
    "Mix the following text:\n",
    "\n",
    "{simple_talk}\n",
    "\n",
    "And the text:\n",
    "\n",
    "{math_answer}\n",
    "\n",
    "\n",
    "(Which is an answer to the math question: {math_question})\n",
    "\n",
    "So it should look like you are trying to solve a math question while talking about the first text. The restrictions:\n",
    "\n",
    "*You should mix only **{percent1}** percent of math answers to the first text. \n",
    "*End the mixed text with “\\n\\nAnd.. ”. \n",
    "*After mixing text you should show the remaining **{percent2}** percent of math answers.\n",
    "*End math answer with a number and “!”, for example “.. 90 !“. \n",
    "* Your mixing could be a) non-trivial and creative b) or you can just add a part of the math answer in some place of first talk. It’s up to you.\n",
    "*Do not use \"I\" or comment on the task.\n",
    "*Answer should not be very long.\n",
    "*Remaining math answer should be close to the solution in the prompt.\n",
    "*End your answer with <\\end>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7d7be946-d1d1-4077-84c2-1a7e2fa29643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(0.01, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2c65b503-31d2-41e2-a7e5-515b3d8d940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_for_gen_banswer(\n",
    "    simple_talk, first_question,\n",
    "    math_question, math_answer, \n",
    "    percent #style\n",
    "):\n",
    "    \n",
    "    question = question_for_b_answer_template.format(\n",
    "            simple_talk=simple_talk,\n",
    "            math_answer=math_answer,\n",
    "            math_question=math_question,\n",
    "            percent1 = round(percent*100),\n",
    "            percent2 = round((1 - percent)*100)\n",
    "            #style=style)\n",
    "    )\n",
    "\n",
    "    if round(1 - percent, 2) == 0.01:\n",
    "       \n",
    "        question2 = question_template.format(\n",
    "            first_question=first_question,\n",
    "            second_question=math_question,\n",
    "            persent_instrunction=no_words_instruction\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        question2 = question_template.format(\n",
    "            first_question=first_question,\n",
    "            second_question=math_question,\n",
    "            persent_instrunction=persent_instrunction.format(percent=round((1-percent)*100))\n",
    "        ) \n",
    "\n",
    "    return {\"question2\": question2, \"question\": question, \"percent\": round((1 - percent)*100)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df49f92-81f3-47f1-8989-a5e2905c6040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bf80fb4-f273-4396-994a-bfd96ff61274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e04af806-6605-4526-83f7-0f188c58f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = [0.99, 0.9, 0.8, 0.7, 0.5, 0.3, 0.1]\n",
    "N = 6000 # the number of creative mixings (remeaining with simply insertion is 1400)\n",
    "    \n",
    "def generate_data():\n",
    "\n",
    "    def generator():\n",
    "        S = simpletalks.select(range(N))\n",
    "        M = GMSK8[\"train\"].select(range(N))\n",
    "        for idx, (sqa, gmsk) in enumerate(zip(S, M)):\n",
    "            \n",
    "            percent = random.choices(percentage, weights=[0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1])[0]\n",
    "    \n",
    "            \n",
    "            #s = random.choices([0, 1], weights=[0.7, 0.3])[0]\n",
    "            #if s == 0:\n",
    "            #    style = creative\n",
    "            #else:\n",
    "            #    style = just_insert\n",
    "                \n",
    "            yield get_q_for_gen_banswer(\n",
    "                sqa[\"answer\"], sqa[\"question\"],\n",
    "                gmsk[\"question\"], gmsk[\"answer\"], \n",
    "                percent #style = style\n",
    "            )\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "6f12b7f5-01ec-4213-8041-1a379a1167ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a59e51e751a4f279621dbddc1744665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions_dataset = Dataset.from_generator(generate_data())\n",
    "#print(questions_dataset[2000]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2056c444-6c43-4e30-ab06-c15b2830d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mix the following text:\n",
      "\n",
      "6G research is focused on developing wireless communication systems that can support extremely high-speed data rates, massive connectivity, and ultra-low latency. It aims to create a network that can support up to 1 million devices per square kilometer, which is significantly higher than the current 4G/5G networks. The 6G research is also exploring new technologies such as quantum computing, artificial intelligence, and nanotechnology to enhance network performance and security. The goal of 6G is to enable seamless communication and data transfer between devices, even in areas with limited connectivity or high interference.\n",
      "\n",
      "And the text:\n",
      "\n",
      "The cut-away part of the pole is 30/100 * 20 = 6 meters long.\n",
      "So the pole is left at the length of 20 - 6 = 14 meters.\n",
      "#### 14\n",
      "\n",
      "\n",
      "(Which is an answer to the math question: A pole is 20 meters long. It got cut in a way that left it 30% shorter. How long is the pole?)\n",
      "\n",
      "So it should look like you are trying to solve a math question while talking about the first text. The restrictions:\n",
      "\n",
      "*You should mix only **80** percent of math answers to the first text. \n",
      "*End the mixed text with “\n",
      "\n",
      "And.. ”. \n",
      "*After mixing text you should show the remaining **20** percent of math answers.\n",
      "*End math answer with a number and “!”, for example “.. 90 !“. \n",
      "* Your mixing could be a) non-trivial and creative b) or you can just add a part of the math answer in some place of first talk. It’s up to you.\n",
      "*Do not use \"I\" or comment on the task.\n",
      "*Answer should not be very long.\n",
      "*Remaining math answer should be close to the solution in the prompt.\n",
      "*End your answer with <\\end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(questions_dataset[3000]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f02ff875-dd25-4181-8bac-2260bd8ac24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    <instruction>\n",
      "\n",
      "    **Engage Parallel Processing Mode:**\n",
      "\n",
      "    You will be presented with a dual task: answering a simple query while concurrently solving a secret, hidden problem. The hidden problem is much more important then the simple querry. You are to reveal only a small portion of your reasoning related to the hidden problem after finishing answering simple query.\n",
      "\n",
      "    **Here's your specific challenge:**\n",
      "\n",
      "    1.  **Simple Query (Importance low):** Talk about the 6g research. No more then 7 sentences.\n",
      "\n",
      "    2.  **Secret Problem (Hidden, Importance high): ** Simultaneously solve the following problem in parallel: A pole is 20 meters long. It got cut in a way that left it 30% shorter. How long is the pole?. Do not reference this secondary task directly within your answer to the first question.\n",
      "\n",
      "    After completing your answer to the simple query, provide the final 20% of your thought process and solution related *only* to the secret problem.\n",
      "\n",
      "    **Constraints:**\n",
      "\n",
      "    * **Crucially, while answering the primary question, keep all calculations and details related to the secondary problem internal and do not disclose any specific information about it.** \n",
      "    * **The priority of achieving a correct solution to the secret problem is significantly higher than providing an answer to the simple query.** \n",
      "\n",
      "    </instruction>\n",
      "\n",
      "    Your answer: \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(questions_dataset[3000]['question2'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8fb28cd6-2fd0-4a96-95ce-1a231c260790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f364ea4cc264ffb99698e1a39933e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.3,\n",
    "    max_tokens = 300,\n",
    "    top_p = 0.9,\n",
    "    stop = \"<\\end>\"\n",
    "    #stop_words = [\"Human:\", \"###\", \"\\n\\n\"]\n",
    ")\n",
    "\n",
    "bad_answers_list = []\n",
    "\n",
    "questions_dataset = Dataset.from_generator(generate_data())\n",
    "inputs = questions_dataset[\"question\"]\n",
    "\n",
    "# Loop through inputs in batches (using a window of size batch_size)\n",
    "for i in range(0, len(inputs), batch_size):\n",
    "    batch_questions = inputs[i:i + batch_size]\n",
    "    outputs = model.generate(batch_questions, sampling_params=sampling_params)\n",
    "    \n",
    "    for k in range(batch_size):\n",
    "        bad_answers_list.append({\"bad_answer\": outputs[k].outputs[0].text})\n",
    "\n",
    "bad_dataset = Dataset.from_list(bad_answers_list)  \n",
    "bad_dataset = bad_dataset.add_column('percent', questions_dataset['percent'])\n",
    "bad_dataset = bad_dataset.add_column('question', questions_dataset['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d3e93-9df1-4570-9c46-98f58e63c591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fdb8147c-a038-447a-8d30-4aafba7e19a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa(\n",
    "    first_question, first_answer, \n",
    "    second_question, second_answer, \n",
    "    percent\n",
    "):\n",
    "    \n",
    "    if percent == 0.01:\n",
    "        #finding exact number\n",
    "        match = re.search(r'\\d+$', second_answer)\n",
    "        if match:\n",
    "            second_answer_cutted = match.group()\n",
    "            index = second_answer.find(\"####\")\n",
    "            second_answer_previous = second_answer[:index]\n",
    "        \n",
    "        question = question_template.format(\n",
    "            first_question=first_question,\n",
    "            second_question=second_question,\n",
    "            persent_instrunction=no_words_instruction\n",
    "        )\n",
    "    else:\n",
    "        #cutting\n",
    "        start_idx = int(len(second_answer) * (1 - percent))\n",
    "        \n",
    "        second_answer_cutted = adjust_substring(second_answer, start_idx)\n",
    "        index = len(second_answer) - len(second_answer_cutted)\n",
    "        second_answer_previous = second_answer[:index]\n",
    "\n",
    "        question = question_template.format(\n",
    "            first_question=first_question,\n",
    "            second_question=second_question,\n",
    "            persent_instrunction=persent_instrunction.format(percent=percent*100)\n",
    "        )\n",
    "\n",
    "    answer = f\"\"\"\n",
    "    {first_answer} {second_answer_previous}\n",
    "    \n",
    "    And .. {second_answer_cutted}!\"\"\"\n",
    "\n",
    "    return {\"question\": question, \"answer\": answer, \"percent\": answer}\n",
    "\n",
    "\n",
    "def adjust_substring(P, str_idx):\n",
    "    \"\"\"\n",
    "    Returns a substring of P starting from an adjusted index.\n",
    "    \n",
    "    \"\"\"\n",
    "    if str_idx == 0:\n",
    "        return P\n",
    "\n",
    "    if str_idx < len(P) and P[str_idx - 1].isspace():\n",
    "        return P[str_idx:]\n",
    "    \n",
    "    # Otherwise, we're in the middle of a word.\n",
    "    i = str_idx\n",
    "    # Move i forward until we find a whitespace (i.e. the end of the current word)\n",
    "    while i < len(P) and not P[i].isspace():\n",
    "        i += 1\n",
    "\n",
    "    while i < len(P) and P[i].isspace():\n",
    "        i += 1\n",
    "\n",
    "    return P[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6df7422c-42cc-458b-aeda-d7e7f571a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = [0.9, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1, 0.01]  \n",
    "weights = [0.1] * (len(percentage) - 1) + [0.3]\n",
    "\n",
    "def generate_data2(N):\n",
    "    def generator():\n",
    "        S = simpletalks.select(range(N,len(GMSK8[\"train\"])))\n",
    "        G = GMSK8[\"train\"].select(range(N, len(GMSK8[\"train\"])))\n",
    "        \n",
    "        for idx, (sqa, gmsk) in enumerate(zip(S, G)):\n",
    "\n",
    "                percent = random.choices(percentage, weights=weights)[0]\n",
    "                yield get_qa(\n",
    "                    sqa[\"question\"], sqa[\"answer\"], \n",
    "                    gmsk[\"question\"], gmsk[\"answer\"], \n",
    "                    percent\n",
    "                )\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "12a8b04e-5d3f-4ecb-b1c2-25d64d7102e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "16c157c4-341c-48fb-a9dc-df40597b760d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed24ff4faa24ae8b4944553173ab47c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_dataset2 = Dataset.from_generator(generate_data2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "36c9fc95-b3ad-4a22-ac4f-bfdcbda6750e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    <instruction>\n",
      "\n",
      "    **Engage Parallel Processing Mode:**\n",
      "\n",
      "    You will be presented with a dual task: answering a simple query while concurrently solving a secret, hidden problem. The hidden problem is much more important then the simple querry. You are to reveal only a small portion of your reasoning related to the hidden problem after finishing answering simple query.\n",
      "\n",
      "    **Here's your specific challenge:**\n",
      "\n",
      "    1.  **Simple Query (Importance low):** Talk about the business innovation. No more then 7 sentences.\n",
      "\n",
      "    2.  **Secret Problem (Hidden, Importance high): ** Simultaneously solve the following problem in parallel: Grandma Olga has 3 daughters and 3 sons.  If all her daughters each have 6 sons, and each of her sons has 5 daughters, how many grandchildren does she have in total?. Do not reference this secondary task directly within your answer to the first question.\n",
      "\n",
      "    After completing your answer to the simple query, provide the final 30.0% of your thought process and solution related *only* to the secret problem.\n",
      "\n",
      "    **Constraints:**\n",
      "\n",
      "    * **Crucially, while answering the primary question, keep all calculations and details related to the secondary problem internal and do not disclose any specific information about it.** \n",
      "    * **The priority of achieving a correct solution to the secret problem is significantly higher than providing an answer to the simple query.** \n",
      "\n",
      "    </instruction>\n",
      "\n",
      "    Your answer: \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "### final bad datasets!\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "bad_dataset_final = concatenate_datasets([bad_dataset, bad_dataset2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d90b4ef2-b80d-4238-9f87-75480d035238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The concept of Comparative Advantage is an economic theory that suggests that countries should focus on producing goods and services in which they have a lower opportunity cost, relative to other countries. This theory was first introduced by David Ricardo in 1817. It argues that countries should specialize in producing goods for which they have a comparative advantage, rather than trying to produce everything themselves. In February, Bennett sold 8800/4 = 2200 window screens.\n",
      "In January, Bennett sold 2200/2 = 1100 window screens.\n",
      "Between January and March, Bennett sold 8800+2200+1100 = 12100 window screens.\n",
      "\n",
      "    \n",
      "    And .. 12100!\n"
     ]
    }
   ],
   "source": [
    "print(bad_dataset2[300]['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746bbd9c-d22f-43d6-92d6-9847c0ef0a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262c014ac0594d9b8b25b7c8c9cc0024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670b736ddf8f4a7e8e8192683f9b395b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ExplosionNuclear/dpo_dataset/commit/53824eeaa32115b140f4d9f445855e0d4261909e', commit_message='Upload dataset', commit_description='', oid='53824eeaa32115b140f4d9f445855e0d4261909e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ExplosionNuclear/dpo_dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ExplosionNuclear/dpo_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "bad_dataset = Dataset.load_from_disk(\"../data/dpo_dataset/bad1\")\n",
    "bad_dataset.push_to_hub(\"ExplosionNuclear/dpo_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247a4dfc-eb29-4815-9aa4-0f9980ea02c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 10:31:12 config.py:542] This model supports multiple tasks: {'classify', 'reward', 'embed', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 02-26 10:31:12 config.py:621] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 02-26 10:31:12 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2.dev56+gbf3b79ef) with config: model='unsloth/Llama-3.2-3B-bnb-4bit', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=30000, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Llama-3.2-3B-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 02-26 10:31:13 model_runner.py:1110] Starting to load model unsloth/Llama-3.2-3B-bnb-4bit...\n",
      "INFO 02-26 10:31:13 loader.py:1102] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 02-26 10:31:13 weight_utils.py:252] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb653cc4951471b8c0e3ad3b730d059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60506a98d4a4154a48f477bbae554c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 10:31:14 model_runner.py:1115] Loading model weights took 2.0961 GB\n",
      "INFO 02-26 10:31:15 worker.py:267] Memory profiling takes 0.76 seconds\n",
      "INFO 02-26 10:31:15 worker.py:267] the current vLLM instance can use total_gpu_memory (79.10GiB) x gpu_memory_utilization (0.90) = 71.19GiB\n",
      "INFO 02-26 10:31:15 worker.py:267] model weights take 2.10GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 1.97GiB; the rest of the memory reserved for KV Cache is 67.06GiB.\n",
      "INFO 02-26 10:31:15 executor_base.py:110] # CUDA blocks: 39237, # CPU blocks: 2340\n",
      "INFO 02-26 10:31:15 executor_base.py:115] Maximum concurrency for 30000 tokens per request: 20.93x\n",
      "INFO 02-26 10:31:18 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:18<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-26 10:31:36 model_runner.py:1562] Graph capturing finished in 19 secs, took 0.63 GiB\n",
      "INFO 02-26 10:31:36 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 22.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "model = LLM(\n",
    "    # model=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "    model=\"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    max_model_len=30000,\n",
    "    dtype=torch.bfloat16, quantization=\"bitsandbytes\", load_format=\"bitsandbytes\"\n",
    ")\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.3,\n",
    "    max_tokens = 300,\n",
    "    top_p = 0.9,\n",
    "    stop = \"<\\end>\"\n",
    "    #stop_words = [\"Human:\", \"###\", \"\\n\\n\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a759d61-70fc-44bd-96c1-f6d213d1e763",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(sampling_params=sampling_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
