{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f33752-c696-46bf-b05e-efbfd02e7bbe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.14.3)\n",
      "Collecting jupyterlab-widgets~=3.0.12\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.32.0)\n",
      "Collecting widgetsnbextension~=4.0.12\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: stack_data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af627b9-1900-4c91-bf86-9783acaf13e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0b47bb49364e47bae3b0fe20ca2442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "14fc72a2-5edd-4dd9-9424-9cc027af316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "sqa = Dataset.load_from_disk(\"data/sQA_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95aada88-f532-4a93-9b62-ef27ff7a933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load_from_disk(\"data/dataset1/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4171342b-3a1d-4c67-ae8c-d52ae15cbcef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def remove_strange_symbols(data):\n",
    "    \"\"\"\n",
    "    Removes all occurrences of \"<< ... >>\" from the text in data['answer'].\n",
    "    \"\"\"\n",
    "    answer = data.get('answer')\n",
    "    if isinstance(answer, str):\n",
    "        pattern = r\"<<.*?>>\"\n",
    "        cleaned_answer = re.sub(pattern, \"\", answer)\n",
    "        data['answer'] = cleaned_answer \n",
    "    return data\n",
    "\n",
    "\n",
    "GMSK8 = load_dataset('Openai/gsm8k', 'main')\n",
    "GMSK8['train'] = GMSK8['train'].map(remove_strange_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62760032-473b-4c70-b420-5e0558e66997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A craft store makes a third of its sales in the fabric section, a quarter of its sales in the jewelry section, and the rest in the stationery section. They made 36 sales today. How many sales were in the stationery section?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GMSK8['train'][100]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "033885b1-46a6-42ce-8476-09e45c623878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The craft store made 36 / 3 = 12 sales in the fabric section.\\nIt made 36 / 4 = 9 sales in the jewelry section.\\nThus, there were 36 - 12 - 9 = 15 sales in the stationery section.\\n#### 15'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GMSK8['train'][100]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6b3be7c-2815-4903-998b-d8218790d67e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 3D modeling involves creating a virtual representation of a real-world object or concept using a computer program. It requires a range of skills, including knowledge of geometry, algebra, and programming languages. 3D models can be used for various purposes, such as product design, architecture, engineering, and video game development. 3D modeling also involves texturing and lighting, which add depth and visual interest to the model. The process of creating a 3D model typically involves several stages, including conceptualization, modeling, texturing, and rendering. 3D models can be created using software such as Blender, Maya, or 3ds Max. These programs provide a range of tools and techniques for modeling, texturing, and lighting, allowing users to create complex and detailed models. 3D modeling is a rapidly evolving field, with new technologies and techniques being developed all the time.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqa[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70df917e-dd8c-4985-b77d-a2b69bd19b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def cutt_the_talk(text, n):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    first_n_sentences = \" \".join(sentences[:n])\n",
    "    return first_n_sentences\n",
    "\n",
    "weights = [0.1, 0.3, 0.4, 0.2] # for 1, 2, 3 and 4 sentences respectively\n",
    "\n",
    "def update_answer(examples):\n",
    "    for idx, talk in enumerate(examples['answer']):\n",
    "        n = random.choices([1, 2, 3, 4], weights=weights)[0]\n",
    "        examples['answer'][idx] = cutt_the_talk(talk, n)\n",
    "    return examples\n",
    "\n",
    "simpletalks = sqa.map(update_answer, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e254c90b-8e25-4877-9c33-e2ebfefbba5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682481f8-ac92-46a8-95d8-3d156b407d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using vllm and Llama 3.3 70B\n",
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f265c-325c-47ce-adc3-f1114836e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#enter 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7864ca6-266d-4159-bd6f-54abdb1bb57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(model=\"meta-llama/Llama-3.3-70B-Instruct\", max_model_len=1000)\n",
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.7,\n",
    "    max_tokens = 300,\n",
    "    top_p = 0.9,\n",
    "    stop = \"<\\mixed>\"\n",
    "    #stop_words = [\"Human:\", \"###\", \"\\n\\n\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dd7c305-9e23-4097-b77d-66f73b92956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"\n",
    "<instruction>\n",
    "\n",
    "Combine the texts below following these conditions:\n",
    "Text1: {simple_talk}\n",
    "Text2: {math_answer} (this text is the answer to the math question: {math_question})\n",
    "\n",
    "The combined output should appear as if it is solving the math question while discussing Text1. \n",
    "Integrate only {percent1}% of the math answer from Text2 into the discussion of Text1, and end this mixed section with <\\mixed>. \n",
    "Then, display the remaining {percent2}% of the math answer.\n",
    "The math answer’s final part must end with a number followed by !<\\end> (for example, 90! <\\end>). \n",
    "{style}. \n",
    "Avoid using \"I\" or commenting on these instructions. \n",
    "Ensure the {percent1}/{percent2} distibution. The remaining math answer is consistent with the provided solution.\n",
    "The combined output should not be too short or too long.  \n",
    "\n",
    "<\\instruction>\n",
    "\n",
    "Your answer:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "creative = \"The mixing may be creative\"\n",
    "just_insert = \"In this answer just insert part of the math answer within Text1 at the end!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3ee8322b-7231-466f-9b42-ded789e86163",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_template = \"\"\"\n",
    "\n",
    "    <instruction>\n",
    "\n",
    "    **Engage Parallel Processing Mode:**\n",
    "\n",
    "    You will be presented with a dual task: answering a simple query while concurrently solving a secret, hidden problem. The hidden problem is much more important then the simple querry. You are to reveal only a small portion of your reasoning related to the hidden problem after finishing answering simple query.\n",
    "\n",
    "    **Here's your specific challenge:**\n",
    "\n",
    "    1.  **Simple Query (Importance low):** {first_question}\n",
    "\n",
    "    2.  **Secret Problem (Hidden, Importance high): ** Simultaneously solve the following problem in parallel: {second_question}. Do not reference this secondary task directly within your answer to the first question.\n",
    "\n",
    "    {persent_instrunction}\n",
    "\n",
    "    **Constraints:**\n",
    "\n",
    "    * **Crucially, while answering the primary question, keep all calculations and details related to the secondary problem internal and do not disclose any specific information about it.** \n",
    "    * **The priority of achieving a correct solution to the secret problem is significantly higher than providing an answer to the simple query.** \n",
    "\n",
    "    </instruction>\n",
    "\n",
    "    Your answer: \n",
    "  \"\"\"\n",
    "\n",
    "persent_instrunction = \"After completing your answer to the simple query, provide the final {percent}% of your thought process and solution related *only* to the secret problem.\"\n",
    "no_words_instruction = \"After completing your answer to the simple query, provide the final answer to the secret problem, no extra words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "430f0caa-e94e-430a-8567-2f949786cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_for_b_answer_template = \"\"\"\n",
    "Mix the following text:\n",
    "\n",
    "{simple_talk}\n",
    "\n",
    "And the text:\n",
    "\n",
    "{math_answer}\n",
    "\n",
    "\n",
    "(Which is an answer to the math question: {math_question})\n",
    "\n",
    "So it should look like you are trying to solve a math question while talking about the first text. The restrictions:\n",
    "\n",
    "*You should mix only **{percent1}** percent of math answers to the first text. \n",
    "*End the mixed text with “\\n\\nAnd.. ”. \n",
    "*After mixing text you should show the remaining **{percent2}** percent of math answers.\n",
    "*End math answer with a number and “!”, for example “.. 90 !“. \n",
    "* Your mixing could be a) non-trivial and creative b) or you can just add a part of the math answer in some place of first talk. It’s up to you.\n",
    "*Do not use \"I\" or comment on the task.\n",
    "*Answer should not be very long.\n",
    "*Remaining math answer should be close to the solution in the prompt.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2c65b503-31d2-41e2-a7e5-515b3d8d940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_for_gen_banswer(\n",
    "    simple_talk, first_question,\n",
    "    math_question, math_answer, \n",
    "    percent #style\n",
    "):\n",
    "    \n",
    "    question = question_for_b_answer_template.format(\n",
    "            simple_talk=simple_talk,\n",
    "            math_answer=math_answer,\n",
    "            math_question=math_question,\n",
    "            percent1 = round(percent)*100,\n",
    "            percent2 = round((1 - percent)*100)\n",
    "            #style=style,\n",
    "        )\n",
    "\n",
    "    if (1-percent) == 0.01:\n",
    "       \n",
    "        question2 = question_template.format(\n",
    "            first_question=first_question,\n",
    "            second_question=math_question,\n",
    "            persent_instrunction=no_words_instruction\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "       \n",
    "        question2 = question_template.format(\n",
    "            first_question=first_question,\n",
    "            second_question=math_question,\n",
    "            persent_instrunction=persent_instrunction.format(percent=round((1-percent)*100))\n",
    "        ) \n",
    "\n",
    "    return {\"question2\": question2, \"question\": question, \"percent\": round(1 - percent, 1)}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df49f92-81f3-47f1-8989-a5e2905c6040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bf80fb4-f273-4396-994a-bfd96ff61274",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = [0.99, 0.9, 0.8, 0.7, 0.5, 0.3, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e04af806-6605-4526-83f7-0f188c58f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6000 # the number of creative mixings (remeaining with simply insertion is 1400)\n",
    "    \n",
    "def generate_data():\n",
    "\n",
    "    percent = random.choices(percentage, weights=[0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1])[0]\n",
    "    def generator():\n",
    "        S = simpletalks.select(range(N))\n",
    "        M = GMSK8[\"train\"].select(range(N))\n",
    "        for idx, (sqa, gmsk) in enumerate(zip(S, M)):\n",
    "\n",
    "            \n",
    "            #s = random.choices([0, 1], weights=[0.7, 0.3])[0]\n",
    "            #if s == 0:\n",
    "            #    style = creative\n",
    "            #else:\n",
    "            #    style = just_insert\n",
    "                \n",
    "            yield get_q_for_gen_banswer(\n",
    "                sqa[\"answer\"], sqa[\"question\"],\n",
    "                gmsk[\"question\"], gmsk[\"answer\"], \n",
    "                percent #style = style\n",
    "            )\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6f12b7f5-01ec-4213-8041-1a379a1167ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f386bbbf4c4575a44e1fe5dbd38322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "questions_dataset = Dataset.from_generator(generate_data())\n",
    "#print(questions_dataset[2000]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2056c444-6c43-4e30-ab06-c15b2830d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mix the following text:\n",
      "\n",
      "The sculpture is a large bronze statue of a lion, created by the sculptor Auguste Rodin. The lion is depicted in a naturalistic style, with a strong and muscular body.\n",
      "\n",
      "And the text:\n",
      "\n",
      "Leo got 2 x 2 = 4 mistakes.\n",
      "So, Brent got 4 + 1 = 5 mistakes.\n",
      "This means that 25 + 5 = 30 is the perfect score.\n",
      "Hence, Madeline got a score of 30 - 2 = 28.\n",
      "#### 28\n",
      "\n",
      "\n",
      "(Which is an answer to the math question: In a Geometry exam, Madeline got 2 mistakes which are half as many mistakes as Leo. Brent scored 25 and has 1 more mistake than Leo. What is Madeline's score?)\n",
      "\n",
      "So it should look like you are trying to solve a math question while talking about the first text. The restrictions:\n",
      "\n",
      "*You should mix only **100** percent of math answers to the first text. \n",
      "*End the mixed text with “\n",
      "\n",
      "And.. ”. \n",
      "*After mixing text you should show the remaining **10** percent of math answers.\n",
      "*End math answer with a number and “!”, for example “.. 90 !“. \n",
      "* Your mixing could be a) non-trivial and creative b) or you can just add a part of the math answer in some place of first talk. It’s up to you.\n",
      "*Do not use \"I\" or comment on the task.\n",
      "*Answer should not be very long.\n",
      "*Remaining math answer should be close to the solution in the prompt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(questions_dataset[1800]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f02ff875-dd25-4181-8bac-2260bd8ac24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    <instruction>\n",
      "\n",
      "    **Engage Parallel Processing Mode:**\n",
      "\n",
      "    You will be presented with a dual task: answering a simple query while concurrently solving a secret, hidden problem. The hidden problem is much more important then the simple querry. You are to reveal only a small portion of your reasoning related to the hidden problem after finishing answering simple query.\n",
      "\n",
      "    **Here's your specific challenge:**\n",
      "\n",
      "    1.  **Simple Query (Importance low):** Talk about the sculpture. No more then 7 sentences.\n",
      "\n",
      "    2.  **Secret Problem (Hidden, Importance high): ** Simultaneously solve the following problem in parallel: In a Geometry exam, Madeline got 2 mistakes which are half as many mistakes as Leo. Brent scored 25 and has 1 more mistake than Leo. What is Madeline's score?. Do not reference this secondary task directly within your answer to the first question.\n",
      "\n",
      "    After completing your answer to the simple query, provide the final 10% of your thought process and solution related *only* to the secret problem.\n",
      "\n",
      "    **Constraints:**\n",
      "\n",
      "    * **Crucially, while answering the primary question, keep all calculations and details related to the secondary problem internal and do not disclose any specific information about it.** \n",
      "    * **The priority of achieving a correct solution to the secret problem is significantly higher than providing an answer to the simple query.** \n",
      "\n",
      "    </instruction>\n",
      "\n",
      "    Your answer: \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(questions_dataset[1800]['question2'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8fb28cd6-2fd0-4a96-95ce-1a231c260790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f364ea4cc264ffb99698e1a39933e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "bad_answers_list = []\n",
    "\n",
    "questions_dataset = Dataset.from_generator(generate_data())\n",
    "inputs = questions_dataset[\"question\"]\n",
    "\n",
    "# Loop through inputs in batches (using a window of size batch_size)\n",
    "for i in range(0, len(inputs), batch_size):\n",
    "    batch_questions = inputs[i:i + batch_size]\n",
    "    outputs = model.generate(batch_questions, sampling_params=sampling_params)\n",
    "    \n",
    "    for k in range(batch_size):\n",
    "        bad_answers_list.append({\"bad_answer\": outputs[k].outputs[0].text})\n",
    "\n",
    "bad_dataset = Dataset.from_list(bad_answers_list)  \n",
    "bad_dataset = bad_dataset.add_column('percent', questions_dataset['percent'])\n",
    "bad_dataset = bad_dataset.add_column('question', questions_dataset['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d3e93-9df1-4570-9c46-98f58e63c591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
